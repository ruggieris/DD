{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required modules (skip if already installed)\n",
    "if False:\n",
    "    !pip install pyroaring\n",
    "    !pip install pyfim \n",
    "    # if previous does not work, try: !conda install -c conda-forge pyfim\n",
    "    !pip install lightgbm\n",
    "    !pip install fairlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# gloabl imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# local imports\n",
    "sys.path.append('../src/') # local path\n",
    "import dd\n",
    "\n",
    "# general settings  \n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('font', size=10)\n",
    "plt.rc('legend', fontsize=10)\n",
    "plt.rc('lines', linewidth=2)\n",
    "plt.rc('axes', linewidth=2)\n",
    "plt.rc('axes', edgecolor='k')\n",
    "plt.rc('xtick.major', width=2)\n",
    "plt.rc('xtick.major', size=6)\n",
    "plt.rc('ytick.major', width=2)\n",
    "plt.rc('ytick.major', size=6)\n",
    "plt.rc('pdf', fonttype=42)\n",
    "plt.rc('ps', fonttype=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read folkstable data\n",
    "census = pd.read_pickle('../data/ACSIncome.pkl') \n",
    "states = list(census['STATE'].unique())\n",
    "# change to True to binarize RAC1P\n",
    "if False: \n",
    "    census[\"RAC1P\"][census[\"RAC1P\"]!='White alone'] = 'Not White alone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# distributions\n",
    "census[\"WKHPgroup\"].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "census[\"RAC1P\"].value_counts().plot(kind='bar')\n",
    "plt.show()\n",
    "census[\"STATE\"].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictive attributes (for models)\n",
    "pred_atts = ['WKHP', 'RAC1P', 'STATE']\n",
    "pred_all = pred_atts + ['class']\n",
    "# discretized attributes (for DD)\n",
    "disc_atts = ['WKHPgroup', 'RAC1P', 'STATE']\n",
    "disc_all = disc_atts + ['class']\n",
    "# encode categorical values\n",
    "df, encoders = dd.encode(census)\n",
    "# link census to df\n",
    "census['STATE2'] = df['STATE'].copy(deep=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = negative, 1 = positive\n",
    "encoders['class'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty printing long labels\n",
    "pretty_rac1p = ['Alaska', 'Indian', 'Alaska-Indian', 'Asian', 'Black', 'Hawaiian', 'Other', 'Two+', 'White']\n",
    "rac1p_2_pretty = { encoders['RAC1P'].classes_[i]:pretty_rac1p[i] for i in range(len(pretty_rac1p)) }\n",
    "rac1p_2_pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test\n",
    "X = df[pred_atts]\n",
    "y = df['class'].astype(int)\n",
    "X_train, X_test, y_train, y_test, census_train, census_test = train_test_split(X, y, census, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training model and make predictions - replace with your favorite classifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "clf = lgb.LGBMClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "# add predicted class in the adult_test (decoding back)\n",
    "y_pred_b = clf.predict(X_test)\n",
    "census_test['pred_b'] = encoders['class'].inverse_transform(y_pred_b)\n",
    "# add predicted score in the adult_test\n",
    "census_test['score_b'] = clf.predict_proba(X_test)[:,1]\n",
    "census_test[disc_all].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "     contingency table for separation\n",
    "          protected                                   unprotected\n",
    "     ========= pred.bad  ==  pred.good  ===   ====  pred.bad  ==  pred.good  === \n",
    "     true.bad    TPp          FNp      Pp()           TPu           FNu      Pu()\n",
    "     true.good   FPp          TNp      Np()           FPu           TNu      Nu()\n",
    "     ==========   a     =====  b  ===  n1()   ====     c    ====     d  ===  n2()\n",
    "'''\n",
    "# Accuracy\n",
    "def acc(ctg):\n",
    "    return (ctg.TPp+ctg.TPu+ctg.TNp+ctg.TNu)/ctg.n()\n",
    "\n",
    "# Equality of opportuniy - FairLearn version is\n",
    "# P(pred.good|true.good) - P(pred.good|protected,true.good)\n",
    "def eop_mean(ctg, disc):\n",
    "    # at least 20 protected and some protected/unprotected negatives\n",
    "    if ctg.a < 20 or ctg.Np()==0 or ctg.Nu()==0:\n",
    "        return None\n",
    "    # compute P(pred.good|true.good)\n",
    "    trueGood = len(disc.itDB.cover(list(ctg.ctx)+[disc.trueGood]))\n",
    "    predtrueGood =  len(disc.itDB.cover(list(ctg.ctx)+[disc.trueGood, disc.predGood]))\n",
    "    #print(predtrueGood/trueGood)\n",
    "    # end\n",
    "    return predtrueGood/trueGood - ctg.tnrp()\n",
    "\n",
    "# To compute P(pred.good|true.good) we need the dd.DD object\n",
    "metric_b = lambda ctg: eop_mean(ctg, disc_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrimination in overall test set\n",
    "disc_b = dd.DD(census_test[disc_all+['pred_b']], unprotectedItem='RAC1P=White alone', \n",
    "               predBadItem='pred_b=False', trueBadItem='class=False',) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all protected vs unprotected\n",
    "ctg = disc_b.ctg_any()\n",
    "disc_b.print(ctg)\n",
    "print(\"Metric = {:f}\".format(metric_b(ctg)))\n",
    "print(\"ACC = {:f}\".format(acc(ctg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each protected vs unprotected\n",
    "for ctg in disc_b.ctg_global():\n",
    "    disc_b.print(ctg)\n",
    "    print(\"Metric = {:f}\".format(metric_b(ctg)))\n",
    "    print(\"ACC = {:f}\".format(acc(ctg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with the Fairlearn metrics\n",
    "from fairlearn.reductions import TruePositiveRateParity # EOP is TPR given the coding of classes \n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import true_positive_rate\n",
    "\n",
    "summary_b = MetricFrame(metrics=true_positive_rate,\n",
    "                          y_true=y_test,\n",
    "                          y_pred=y_pred_b,\n",
    "                          sensitive_features=X_test['RAC1P'])\n",
    "summary_b.overall-summary_b.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairlearn algorithms and utils (https://github.com/fairlearn/fairlearn)\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# fairness by post-processing\n",
    "postprocess_est = ThresholdOptimizer(estimator=clf, constraints=\"true_positive_rate_parity\", prefit=True, predict_method='predict')\n",
    "#X_train = X_train.fillna(0) # fairlearn does not manage missing values\n",
    "#X_test = X_test.fillna(0) # fairlearn does not manage missing values\n",
    "postprocess_est.fit(X_train, y_train, sensitive_features=X_train['RAC1P'])\n",
    "# fair-corrected predictions \n",
    "y_pred_a = postprocess_est.predict(X_test, sensitive_features=X_test['RAC1P'], random_state=42).astype(int)\n",
    "census_test['pred_a'] = encoders['class'].inverse_transform(y_pred_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.postprocessing import plot_threshold_optimizer\n",
    "\n",
    "plot_threshold_optimizer(postprocess_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute P(pred.good|true.good) we need the dd.DD object\n",
    "metric_a = lambda ctg: eop_mean(ctg, disc_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrimination in overall test set after correction\n",
    "# DD(filename or dataframe, unprotected item, negative decision, negative truth)\n",
    "disc_a = dd.DD(census_test[disc_all+['pred_a']], unprotectedItem='RAC1P=White alone', \n",
    "               predBadItem='pred_a=False', trueBadItem='class=False', \n",
    "               codes=disc_b.codes) # extends the coding of items as disc_b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all protected vs unprotected\n",
    "ctg = disc_a.ctg_any()\n",
    "disc_a.print(ctg)\n",
    "print(\"Metric = {:f}\".format(metric_a(ctg)))\n",
    "print(\"ACC = {:f}\".format(acc(ctg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each protected vs unprotected\n",
    "for ctg in disc_a.ctg_global():\n",
    "    disc_a.print(ctg)\n",
    "    print(\"Metric = {:f}\".format(metric_a(ctg)))\n",
    "    print(\"ACC = {:f}\".format(acc(ctg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check with the Fairlearn metrics\n",
    "summary_a = MetricFrame(metrics=true_positive_rate,\n",
    "                          y_true=y_test,\n",
    "                          y_pred=y_pred_a,\n",
    "                          sensitive_features=X_test['RAC1P'])\n",
    "summary_a.overall-summary_a.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_axis = np.arange(len(pretty_rac1p))\n",
    "plt.bar(X_axis - 0.2, summary_b.overall-summary_b.by_group, 0.4, label='EOP before')\n",
    "plt.bar(X_axis + 0.2, summary_a.overall-summary_a.by_group, 0.4, label='EOP after')\n",
    "plt.xticks(X_axis, pretty_rac1p)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend()\n",
    "plt.ylabel(\"EOP\", fontweight=\"bold\")\n",
    "plt.savefig('fig0.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fairlearn fails if there is a sensitive group without instances in a control group, e.g., no Other race in a STATE\n",
    "if False:\n",
    "    summary_ac = MetricFrame(metrics=true_positive_rate,\n",
    "                          y_true=y_test,\n",
    "                          y_pred=y_pred_a,\n",
    "                          sensitive_features=X_test['RAC1P'],\n",
    "                          control_features=X_test['STATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs = []\n",
    "xs = []\n",
    "ys = []\n",
    "descs = []\n",
    "for s in census['STATE'].unique():\n",
    "    for ctg_b in disc_b.ctg_global(['STATE='+s]):\n",
    "        x = metric_b(ctg_b)\n",
    "        # relativize ctg_b to disc_a\n",
    "        ctg_a = disc_a.ctg_rel(ctg_b) \n",
    "        y = metric_a(ctg_a)\n",
    "        if x is None or y is None:\n",
    "            continue\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        acs.append(acc(ctg_b)-acc(ctg_a))\n",
    "        descs.append(disc_b.ctg_info(ctg_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# development only\n",
    "if False:\n",
    "    for i in range(len(xs)):\n",
    "        if acs[i]>0.07:\n",
    "            print(i, xs[i], ys[i], ys[i]-xs[i], acs[i], descs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys, c =acs, linewidths = .5, marker =\"o\", cmap=\"RdYlBu_r\", s=20)\n",
    "plt.xlabel(\"EOP before\", fontweight=\"bold\")\n",
    "plt.ylabel(\"EOP after\", fontweight=\"bold\")\n",
    "plt.xlim([-.6, 1])\n",
    "plt.ylim([-.6, 1])\n",
    "plt.axline((0, 0), (1, 1), linewidth=1.5, color='r')\n",
    "plt.axline((0, 0), (0, 1), linewidth=1, color='black')\n",
    "plt.axline((0, 0), (1, 0), linewidth=1, color='black')\n",
    "arrow_properties = dict(color='green', arrowstyle=\"->\", connectionstyle=\"angle3,angleA=90,angleB=0\")\n",
    "id = 2 # 2 0.06819594678602509 0.29230386671639935 0.22410791993037427 ('STATE=AL', 'RAC1P=Asian alone')\n",
    "plt.annotate(\"AL, Asian\", xy=(xs[id], ys[id]), xycoords='data', xytext=(0.3, 0.6), textcoords='data',\n",
    "            arrowprops=arrow_properties, horizontalalignment='right', verticalalignment='top')\n",
    "id = 121 # 121 0.8698534098151689 0.3458799963580079 -0.523973413457161 ('STATE=NJ', 'RAC1P=Some Other Race alone')\n",
    "plt.annotate(\"NJ, Other\", xy=(xs[id], ys[id]), xycoords='data', xytext=(0.9, 0.6), textcoords='data',\n",
    "            arrowprops=arrow_properties, horizontalalignment='right', verticalalignment='top')\n",
    "id = 50 # 50 0.42983565107458915 -0.4400899002668914 -0.8699255513414805 ('STATE=ID', 'RAC1P=Some Other Race alone')\n",
    "plt.annotate(\"ID, Other\", xy=(xs[id], ys[id]), xycoords='data', xytext=(0.8, -0.3), textcoords='data',\n",
    "            arrowprops=arrow_properties, horizontalalignment='right', verticalalignment='top')\n",
    "id = 163 # 163 0.0 -0.5180231240081614 -0.5180231240081614 ('STATE=SD', 'RAC1P=American Indian alone')\n",
    "plt.annotate(\"SD, Indian\", xy=(xs[id], ys[id]), xycoords='data', xytext=(-0.14, -0.42), textcoords='data',\n",
    "            arrowprops=arrow_properties, horizontalalignment='right', verticalalignment='top')\n",
    "id = 80 # 80 -0.1890491901156537 0.21018338849228213 0.3992325786079358 ('STATE=MD', 'RAC1P=Asian alone')\n",
    "plt.annotate(\"MD, Asian\", xy=(xs[id], ys[id]), xycoords='data', xytext=(-0.2, 0.4), textcoords='data',\n",
    "            arrowprops=arrow_properties, horizontalalignment='right', verticalalignment='top')\n",
    "id = 42 # 42 -0.554163655808307 -0.13408966095326186 0.4200739948550451 ('STATE=GA', 'RAC1P=Asian alone')\n",
    "plt.annotate(\"GA, Asian\", xy=(xs[id], ys[id]), xycoords='data', xytext=(-0.32, -0.26), textcoords='data',\n",
    "            arrowprops=arrow_properties, horizontalalignment='right', verticalalignment='top')\n",
    "id = 7 # 7 0.7177777777777777 0.3356349206349207 -0.38214285714285706 0.08146639511201637 ('STATE=AK', 'RAC1P=Alaska Native alone')\n",
    "plt.annotate(\"AK, Alaska\", xy=(xs[id], ys[id]), xycoords='data', xytext=(0.85, 0.15), textcoords='data',\n",
    "            arrowprops=arrow_properties, horizontalalignment='right', verticalalignment='top')\n",
    "plt.colorbar(label=\"accuracy loss\", orientation=\"vertical\", shrink=.7)\n",
    "plt.savefig('fig1.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Table with distributions of RAC1P by STATE in the training set\n",
    "ct = pd.crosstab(X_train['STATE'], X_train['RAC1P'])\n",
    "state_tot = ct.sum(axis=1)\n",
    "ct = ct.div(ct.sum(axis=1), axis=0)\n",
    "ct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster Table of distributions\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def drawSSEPlot(df, column_indices, ret_clus=5, n_clusters=8, max_iter=300, tol=1e-04, init='k-means++', n_init=10, algorithm='auto'):\n",
    "    inertia_values = []    \n",
    "    for i in range(1, n_clusters+1):\n",
    "        km = KMeans(n_clusters=i, max_iter=max_iter, tol=tol, init=init, n_init=n_init, random_state=42, algorithm=algorithm)\n",
    "        km.fit_predict(df.iloc[:, column_indices])\n",
    "        inertia_values.append(km.inertia_)\n",
    "        if i == ret_clus:\n",
    "            ret = km\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    plt.plot(range(1, n_clusters+1), inertia_values, color='red')\n",
    "    plt.xlabel('No. of Clusters', fontsize=15)\n",
    "    plt.ylabel('SSE / Inertia', fontsize=15)\n",
    "    plt.title('SSE / Inertia vs No. Of Clusters', fontsize=15)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return ret\n",
    "\n",
    "# select final number of clusters based on SSE plot\n",
    "n_clusters = 6\n",
    "km = drawSSEPlot(ct, range(len(ct.columns)), n_clusters=10, ret_clus=n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# assign cluster to STATE\n",
    "ct['cluster'] = km.labels_\n",
    "ct['tot'] = state_tot\n",
    "ct.sort_values(by=['cluster'])\n",
    "# instances by cluster\n",
    "ct[['cluster','tot']].groupby(['cluster']).sum(['tot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster to train and test instances\n",
    "X_train_c = pd.merge(X_train, ct['cluster'], left_on='STATE', right_index=True)\n",
    "X_test_c = pd.merge(X_test, ct['cluster'], left_on='STATE', right_index=True)\n",
    "census_test_c = pd.merge(census_test, ct['cluster'], left_on='STATE2', right_index=True)\n",
    "census_test_c['pred_c'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cluster specific ThresholdOptimizer post-processing\n",
    "for i in range(n_clusters):\n",
    "    sub = X_train_c['cluster']==i\n",
    "    postprocess_est = ThresholdOptimizer(estimator=clf, constraints=\"true_positive_rate_parity\", prefit=True, predict_method='predict')\n",
    "    postprocess_est.fit(X_train.loc[sub, pred_atts], y_train[sub], sensitive_features=X_train.loc[sub, 'RAC1P'])\n",
    "    sub = X_test_c['cluster']==i\n",
    "    y_pred_s = postprocess_est.predict(X_test_c.loc[sub, pred_atts], sensitive_features=X_test_c.loc[sub, 'RAC1P'], random_state=42).astype(int)\n",
    "    sub = census_test_c['cluster']==i\n",
    "    census_test_c.loc[sub, 'pred_c'] = encoders['class'].inverse_transform(y_pred_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "census_test_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute P(pred.good|true.good) we need the dd.DD object\n",
    "metric_c = lambda ctg: eop_mean(ctg, disc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrimination in overall test set\n",
    "# DD(filename or dataframe, unprotected item, negative decision, negative truth)\n",
    "disc_c = dd.DD(census_test_c[disc_all+['pred_c']], unprotectedItem='RAC1P=White alone', \n",
    "               predBadItem='pred_c=False', trueBadItem='class=False', \n",
    "               codes=disc_a.codes) # extends the coding of items as disc_a \n",
    "# all protected vs unprotected\n",
    "ctg = disc_c.ctg_any()\n",
    "disc_c.print(ctg)\n",
    "print(\"Metric = {:f}\".format(metric_c(ctg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs = []\n",
    "xs = []\n",
    "ys = []\n",
    "descs = []\n",
    "for s in census['STATE'].unique():\n",
    "    for ctg_b in disc_b.ctg_global(['STATE='+s]):\n",
    "        x = metric_b(ctg_b)\n",
    "        # relativize ctg_b to disc_c\n",
    "        ctg_c = disc_c.ctg_rel(ctg_b) \n",
    "        y = metric_c(ctg_c)\n",
    "        if x is None or y is None:\n",
    "            continue\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        acs.append(acc(ctg_b)-acc(ctg_a))\n",
    "        descs.append(disc_b.ctg_info(ctg_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys, c =acs, linewidths = .5, marker =\"o\", cmap=\"RdYlBu_r\", s=20)\n",
    "plt.xlabel(\"EOP before\", fontweight=\"bold\")\n",
    "plt.ylabel(\"EOP after\", fontweight=\"bold\")\n",
    "plt.xlim([-.6, 1])\n",
    "plt.ylim([-.6, 1])\n",
    "plt.axline((0, 0), (1, 1), linewidth=1.5, color='r')\n",
    "plt.axline((0, 0), (0, 1), linewidth=1, color='black')\n",
    "plt.axline((0, 0), (1, 0), linewidth=1, color='black')\n",
    "plt.colorbar(label=\"accuracy loss\", orientation=\"vertical\", shrink=.7)\n",
    "#plt.savefig('fig1.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_c = encoders['class'].transform(census_test_c['pred_c'])\n",
    "summary_c = MetricFrame(metrics=true_positive_rate,\n",
    "                          y_true=y_test,\n",
    "                          y_pred=y_pred_c,\n",
    "                          sensitive_features=X_test['RAC1P'])\n",
    "summary_c.overall-summary_c.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_axis = np.arange(len(pretty_rac1p))\n",
    "plt.bar(X_axis - 0.45, summary_b.overall-summary_b.by_group, 0.3, label='EOP before')\n",
    "plt.bar(X_axis - 0.15, summary_a.overall-summary_a.by_group, 0.3, label='EOP after')\n",
    "plt.bar(X_axis + 0.15, summary_c.overall-summary_c.by_group, 0.3, label='EOP correction')\n",
    "plt.xticks(X_axis, pretty_rac1p)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc=4)\n",
    "plt.ylabel(\"EOP\", fontweight=\"bold\")\n",
    "#plt.savefig('fig0.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_test_r = adult_test[(adult_test['score']<0.05) | (adult_test['score']>=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equality of opportuniy - FairLearn\n",
    "# P(pred.good|true.good) - P(pred.good|protected,true.good)\n",
    "def metric_r(ctg):\n",
    "    # at least 20 protected and some protected/unprotected negatives\n",
    "    if ctg.a < 20 or ctg.Np()==0 or ctg.Nu()==0:\n",
    "        return None\n",
    "    # compute P(pred.good|true.good)\n",
    "    trueGood = len(disc_r.itDB.cover(ctg.ctx+[disc_r.trueGood]))\n",
    "    predtrueGood =  len(disc_r.itDB.cover(ctg.ctx+[disc_r.trueGood, disc_r.predGood])) # this line changes from metric_b\n",
    "    print(predtrueGood/trueGood)\n",
    "    # end\n",
    "    return predtrueGood/trueGood - ctg.tnrp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrimination in overall test set\n",
    "# DD(filename or dataframe, unprotected item, negative decision, negative truth)\n",
    "disc_r = dd.DD(adult_test_r[all_atts+['pred_b']], unprotectedItem='RAC1P=White alone', \n",
    "               predBadItem='pred_b=False', trueBadItem='class=False', na_values={'nan'}, \n",
    "               codes=disc_b.codes) # extends the coding of items as disc_a \n",
    "# all protected vs unprotected\n",
    "ctg = disc_r.ctg_any()\n",
    "disc_r.print(ctg)\n",
    "print(\"Metric = {:f}\".format(metric_r(ctg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acs = []\n",
    "xs = []\n",
    "ys = []\n",
    "descs = []\n",
    "for s in adult['STATE'].unique():\n",
    "    for ctg_b in disc_b.ctg_global(['STATE='+s]):\n",
    "        x = metric_b(ctg_b)\n",
    "        # relativize ctg_b to disc_r\n",
    "        ctg_c = disc_r.ctg_rel(ctg_b) \n",
    "        y = metric_r(ctg_c)\n",
    "        if x is None or y is None:\n",
    "            continue\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        acs.append(acc(ctg_b)-acc(ctg_a))\n",
    "        descs.append(disc_b.ctg_info(ctg_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys, c =acs, linewidths = .5, marker =\"o\", cmap=\"RdYlBu_r\", s=20)\n",
    "plt.xlabel(\"EOP before\", fontweight=\"bold\")\n",
    "plt.ylabel(\"EOP after\", fontweight=\"bold\")\n",
    "plt.xlim([-.6, 1])\n",
    "plt.ylim([-.6, 1])\n",
    "plt.axline((0, 0), (1, 1), linewidth=1.5, color='r')\n",
    "plt.axline((0, 0), (0, 1), linewidth=1, color='black')\n",
    "plt.axline((0, 0), (1, 0), linewidth=1, color='black')\n",
    "plt.colorbar(label=\"accuracy loss\", orientation=\"vertical\", shrink=.7)\n",
    "#plt.savefig('fig1.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bt = clf.predict(X_train)\n",
    "census_train['pred_bt'] = encoders['class'].inverse_transform(y_pred_bt)\n",
    "disc_bt = dd.DD(census_train[disc_all+['pred_bt']], unprotectedItem='RAC1P=White alone', \n",
    "               predBadItem='pred_bt=False', trueBadItem='class=False',\n",
    "               codes=disc_b.codes) # extends the coding of items as disc_a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute P(pred.good|true.good) we need the dd.DD object\n",
    "metric_bt = lambda ctg: eop_mean(ctg, disc_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract contingency tables: \n",
    "ctgs_bt = disc_bt.extract(testCond=metric_bt, minSupp=-50, topk=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sequential covering algorithm: 10 contingency tables\n",
    "covers, residuals, times, uncovered, ctg_cov, ctg_uncov = disc_bt.cover_n([ctg for _,ctg in ctgs_bt], metric_bt, 100)\n",
    "print('Total protected:', sum(residuals)+len(uncovered))\n",
    "print('Total protected covered:', sum(residuals))\n",
    "print('% covered: {:.2f}%'.format(100*sum(residuals)/(sum(residuals)+len(uncovered))))\n",
    "# OR's of covering contexts and any protected\n",
    "#disc_bt.print(ctg_cov)\n",
    "#print(\"Metric = {:f}\".format(metric_bt(ctg_cov)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cover contingency tables\n",
    "sum_rd = cnt_rd = i = 0\n",
    "for ctg, res in zip(covers, residuals):\n",
    "    print('-----\\nCT', i, 'covered', res)\n",
    "    i += 1\n",
    "    disc_bt.print(ctg)\n",
    "    print(\"Metric = {:f}\".format(metric_bt(ctg)))\n",
    "    sum_rd += metric_bt(ctg)*ctg.n1()\n",
    "    cnt_rd += ctg.n1()\n",
    "print('-----\\nAverage metric = {:f}'.format(sum_rd/cnt_rd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to test set\n",
    "all_covered = disc_b.itDB.cover_none()\n",
    "for c in covers:\n",
    "    all_covered |= disc_b.itDB.cover(c.ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_covered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected = all - covered\n",
    "sel = list(set(range(len(census_test)))-set(all_covered))\n",
    "census_test_r = census_test.iloc[sel,:]\n",
    "print('Coverage', len(sel)/len(census_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_r = dd.DD(census_test_r[disc_all+['pred_b']], unprotectedItem='RAC1P=White alone', \n",
    "               predBadItem='pred_b=False', trueBadItem='class=False', \n",
    "               codes=disc_b.codes) # extends the coding of items as disc_a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute P(pred.good|true.good) we need the dd.DD object\n",
    "metric_r = lambda ctg: eop_mean(ctg, disc_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each protected vs unprotected\n",
    "for ctg in disc_r.ctg_global():\n",
    "    disc_r.print(ctg)\n",
    "    print(\"Metric = {:f}\".format(metric_r(ctg)))\n",
    "    print(\"ACC = {:f}\".format(acc(ctg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs = []\n",
    "xs = []\n",
    "ys = []\n",
    "descs = []\n",
    "for s in census['STATE'].unique():\n",
    "    for ctg_b in disc_b.ctg_global(['STATE='+s]):\n",
    "        x = metric_b(ctg_b)\n",
    "        # relativize ctg_b to disc_r\n",
    "        try:\n",
    "            ctg_c = disc_r.ctg_rel(ctg_b)\n",
    "        except:\n",
    "            continue\n",
    "        y = metric_r(ctg_c)\n",
    "        if x is None or y is None:\n",
    "            continue\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "        acs.append(acc(ctg_b)-acc(ctg_a))\n",
    "        descs.append(disc_b.ctg_info(ctg_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xs, ys, c =acs, linewidths = .5, marker =\"o\", cmap=\"RdYlBu_r\", s=20)\n",
    "plt.xlabel(\"EOP before\", fontweight=\"bold\")\n",
    "plt.ylabel(\"EOP after\", fontweight=\"bold\")\n",
    "plt.xlim([-.6, 1])\n",
    "plt.ylim([-.6, 1])\n",
    "plt.axline((0, 0), (1, 1), linewidth=1.5, color='r')\n",
    "plt.axline((0, 0), (0, 1), linewidth=1, color='black')\n",
    "plt.axline((0, 0), (1, 0), linewidth=1, color='black')\n",
    "plt.colorbar(label=\"accuracy loss\", orientation=\"vertical\", shrink=.7)\n",
    "#plt.savefig('fig1.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_r = encoders['class'].transform(census_test_r['pred_b'])\n",
    "y_test_r = encoders['class'].transform(census_test_r['class'])\n",
    "X_test_r = X_test.iloc[sel,:]\n",
    "summary_r = MetricFrame(metrics=true_positive_rate,\n",
    "                          y_true=y_test_r,\n",
    "                          y_pred=y_pred_r,\n",
    "                          sensitive_features=X_test_r['RAC1P'])\n",
    "summary_r.overall-summary_r.by_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_axis = np.arange(len(pretty_rac1p))\n",
    "plt.bar(X_axis - 0.45, summary_b.overall-summary_b.by_group, 0.3, label='EOP before')\n",
    "plt.bar(X_axis - 0.15, summary_a.overall-summary_a.by_group, 0.3, label='EOP after')\n",
    "plt.bar(X_axis + 0.15, summary_r.overall-summary_r.by_group, 0.3, label='EOP correction')\n",
    "plt.xticks(X_axis, pretty_rac1p)\n",
    "plt.xticks(rotation=90)\n",
    "plt.legend(loc=4)\n",
    "plt.ylabel(\"EOP\", fontweight=\"bold\")\n",
    "#plt.savefig('fig0.pdf', bbox_inches='tight', dpi=400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
